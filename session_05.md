# Demystifying Terms Data Science, Machine Learning, Artificial Intelligence
## Artificial Intelligence
Artificial intelligence (AI) is the ability of machines to learn, think, and perform tasks, especially computer systems. AI uses technologies like machine learning and neural networks to analyze data, make decisions, and more. 
Types of AI
1) Machine learning: A subset of AI that uses algorithms to learn from data and improve over time 
2) Deep learning: A subset of machine learning that uses neural networks to make predictions 
3) Generative AI: A type of AI that can create new content like text, images, or videos based on user prompts 
4) Natural language processing: A type of AI that enables computers to understand and use human language
   
## Artificial General Intelligence
Artificial general intelligence (AGI) is a hypothetical machine intelligence that can learn and perform any intellectual task a human can. AGI is also known as strong AI or deep AI. 

How AGI might work
1) Learn human behavior: AGI can learn human behavior and understand consciousness. 
2) Transfer knowledge: AGI can transfer knowledge and skills learned in one domain to another. 
3) Common sense: AGI has a vast knowledge of the world, including facts, relationships, and social norms. 
4) Social and emotional engagement: AGI can recognize and understand emotions, including facial expressions, body language, and tone of voice.
   
Potential uses of AGI 
1) Assisting healthcare workers
2) Making recommendations for movies, music, and books
3) Contributing to the development of new products and industries
4) Increasing the abundance of resources
5) Turbocharging the global economy
6) Discovering new scientific knowledge

## History of Artificial Intelligence
A survey of important events and people in the field of artificial intelligence (AI) from the early work of British logician Alan Turing in the 1930s to advancements at the turn of the 21st century. AI is the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings. The term is frequently applied to the project of developing systems endowed with the intellectual processes characteristic of humans, such as the ability to reason, discover meaning, generalize, or learn from past experience. For modern developments in AI, see artificial intelligence.

The earliest substantial work in the field of artificial intelligence was done in the mid-20th century by the British logician and computer pioneer Alan Mathison Turing. In 1935 Turing described an abstract computing machine consisting of a limitless memory and a scanner that moves back and forth through the memory, symbol by symbol, reading what it finds and writing further symbols. The actions of the scanner are dictated by a program of instructions that also is stored in the memory in the form of symbols. This is Turing’s stored-program concept, and implicit in it is the possibility of the machine operating on, and so modifying or improving, its own program. Turing’s conception is now known simply as the universal Turing machine. All modern computers are in essence universal Turing machines.

Turing gave quite possibly the earliest public lecture (London, 1947) to mention computer intelligence, saying, “What we want is a machine that can learn from experience,” and that the “possibility of letting the machine alter its own instructions provides the mechanism for this.” In 1948 he introduced many of the central concepts of AI in a report entitled “Intelligent Machinery.” However, Turing did not publish this paper, and many of his ideas were later reinvented by others. For instance, one of Turing’s original ideas was to train a network of artificial neurons to perform specific tasks, an approach described in the section Connectionism.

### Perceptron. 
Perceptron was introduced by Frank Rosenblatt in 1957. He proposed a Perceptron learning rule based on the original MCP neuron. A Perceptron is an algorithm for supervised learning of binary classifiers. This algorithm enables neurons to learn and processes elements in the training set one at a time.

![image](https://github.com/user-attachments/assets/b10a89bd-e5ed-45dd-8f02-616ba3f2bf9d)


### Backpropagation
Backpropagation is a machine learning technique essential to the optimization of artificial neural networks. It facilitates the use of gradient descent algorithms to update network weights, which is how the deep learning models driving modern artificial intelligence (AI) “learn.”

It’s essential to the use of supervised learning, semi-supervised learning or self-supervised learning to train neural networks.

### AI Winter
The "AI Winter": The field faced funding cuts and diminished enthusiasm during the 1970s and 1980s, known as the "AI Winter," due to the inability of AI systems to deliver on the high expectations set earlier.

### DARPA Grand Challenge 2005
The DARPA Grand Challenge 2005 was a pivotal event in the history of autonomous vehicles and artificial intelligence. 
Organized by the Defense Advanced Research Projects Agency (DARPA), the competition aimed to advance the development of self-driving cars. 
It marked a significant step forward in autonomous vehicle technology and showcased the potential of AI and robotics in real-world applications.
Background
The DARPA Grand Challenge was created to encourage research and development in autonomous vehicles, with the goal of creating military robots that could navigate dangerous terrain without human intervention. The competition was designed to push the boundaries of technology and innovation by setting a challenging goal for robotic systems.

The Challenges
The 2005 DARPA Grand Challenge required autonomous vehicles to navigate a 132-mile course across the Mojave Desert in California, filled with obstacles such as hills, sharp turns, and narrow roads. The goal was to complete the race in the shortest time possible, with no human intervention.
The course was designed to be extremely difficult, with both technical and environmental challenges. The vehicles had to navigate through sand, rocks, and other terrain features, all while avoiding other obstacles, without any human control.

Key Takeaways and Impact
1) Technological Advancements: The 2005 challenge pushed the boundaries of AI and robotics, advancing technologies like autonomous navigation, real-time decision-making, and sensor-based systems.

2) Increased Interest in Autonomous Vehicles: The success of vehicles like Stanley brought widespread attention to the potential of self-driving cars, especially in terms of safety, logistics, and defense applications.

3) Inspiration for Future Research: The Grand Challenge was a major stepping stone for research in autonomous driving. It sparked further innovation, with companies like Google (now Waymo) and others focusing on developing self-driving cars.

4) Influence on DARPA's Subsequent Challenges: The DARPA Urban Challenge of 2007, which featured a more complex urban driving environment, was a follow-up to the 2005 event, demonstrating the growing sophistication of autonomous vehicles.


### ImageNet 2012
ImageNet is a large-scale image database created to support research in visual recognition and machine learning. It contains millions of labeled images that are classified into thousands of categories (e.g., animals, vehicles, objects, etc.). Researchers use ImageNet to train and test algorithms for image classification, object detection, and other visual recognition tasks.

### 2024 Nobel Prize in Physics

The 2024 Nobel Prize in Physics was awarded to John J. Hopfield and Geoffrey E. Hinton for their foundational discoveries and inventions that enable machine learning with artificial neural networks. 

John J. Hopfield is an American physicist and emeritus professor at Princeton University. He is renowned for his work on associative neural networks, particularly the Hopfield network, which has been instrumental in pattern recognition and data storage. 

Geoffrey E. Hinton is a British-Canadian computer scientist and professor emeritus at the University of Toronto. Often referred to as the "godfather of AI," Hinton has made significant contributions to the development of deep learning algorithms, particularly in the training of deep neural networks. 

Their pioneering work has been instrumental in the development of artificial intelligence technologies that are now integral to various applications, including image and speech recognition, natural language processing, and autonomous systems.


![image](https://github.com/user-attachments/assets/a9adaabf-2d16-4747-91b9-6790f9fd9429) ![image](https://github.com/user-attachments/assets/36106dae-59ef-4ad8-8e18-ed227e1bb345)


### Deepseek
![image](https://github.com/user-attachments/assets/55023353-adb3-425d-a243-a9b351e4897e)

* A chinese AI firm
* On Jan. 20, 2025, DeepSeek released its R1 LLM at a fraction of the cost that other vendors incurred in their own developments.
* DeepSeek is also providing its R1 models under an open source license, enabling free use.
* DeepSeek focuses on developing open source, meaning they are freely available for anyone to use, modify, and distribute.
* 
## Key Deciding Factors of Growth of AI
1. Huge amount of data (explosion of data)
2. Computing power
3. Algorithmic improvements

## Data Science, Machine Learning and Artificial Intelligence
Data science is a broad field of study about data systems and processes aimed at maintaining data sets and deriving meaning from them. Data scientists use tools, applications, principles, and algorithms to make sense of random data clusters. Since almost all kinds of organizations generate exponential amounts of data worldwide, monitoring and storing this data becomes difficult. Data science focuses on data modeling and warehousing to track the ever-growing data set. The information extracted through data science applications is used to guide business processes and reach organizational goals.

Machine Learning is a subsection of Artificial intelligence that devices mean by which systems can automatically learn and improve from experience. This particular wing of AI aims to equip machines with independent learning techniques so that they don’t have to be programmed. 

Artificial intelligence aims at enabling machines to execute reasoning by replicating human intelligence. Since the main objective of AI processes is to teach machines from experience, feeding the correct information and self-correction is crucial. AI experts rely on deep learning and natural language processing to help machines identify patterns and inferences.
Scope of Artificial Intelligence
1) Automation is easy with AI: AI allows you to automate repetitive, high-volume tasks by setting up reliable systems that run frequent applications.
2) Intelligent Products: AI can turn conventional products into bright commodities. When paired with conversational platforms, bots, and other intelligent machines, AI applications can improve technologies.
3) Progressive Learning: AI algorithms can train machines to perform any desired functions. The algorithms work as predictors and classifiers.
4) Analyzing Data: Since machines learn from the data we feed, analyzing and identifying the correct data set becomes very important. Neural networking makes it easier to train machines.
   
## Data Science Application Domains
1. Healthcare and Medicine
2. Finance and Banking
3. Retail and E-Commerce
4. Marketing and Advertising
5. Transportation and Logistics
6. Energy and Utilities
7. Manufacturing and Industry
8. Telecommunications
9. Sports Analytics
10. Government and Public Sector
11. Education
12. Agriculture

## Correlation Vs. Causation
Correlation refers to a statistical relationship or association between two variables. When two variables are correlated, it means that changes in one variable are associated with changes in another. However, correlation does not imply causation, meaning that just because two variables are correlated, it doesn't necessarily mean that one causes the other to change.

Types of Correlation:
1) Positive Correlation: As one variable increases, the other also increases. For example, there might be a positive correlation between the number of hours studied and test scores.
2) Negative Correlation: As one variable increases, the other decreases. For example, there might be a negative correlation between the amount of time spent watching TV and grades.
3) No Correlation: There is no predictable relationship between the two variables. For example, shoe size and intelligence likely have no correlation.
Examples of Correlation:
1. Ice Cream Sales and Drowning: Studies might show a positive correlation between ice cream sales and drowning incidents. However, this doesn't mean that buying ice cream causes drowning. The actual cause might be a third factor: summer. During hot weather, more people buy ice cream and also tend to swim more, increasing the chance of drowning.
2. Height and Weight: Taller people are often heavier, showing a correlation between height and weight. However, this doesn't mean that being taller causes you to gain weight. It's just a relationship that happens to exist.

How to Measure Correlation:
Correlation is often measured using the Pearson correlation coefficient (denoted by r), which ranges from -1 to +1:

+1: Perfect positive correlation.
-1: Perfect negative correlation.
0: No correlation.

Causation
Causation means that one variable directly causes a change in another. In a causal relationship, one variable's change will result in a change in the other variable. Causality suggests a cause-and-effect relationship, not just an association.

Criteria for Causality (commonly known as Hill's Criteria):
For a causal relationship to be established, certain conditions are typically required:

1) Temporal Sequence: The cause must occur before the effect.
2) Strength: A stronger association between the variables suggests a higher likelihood of causality.
3) Consistency: The relationship should be observed repeatedly in different studies or populations.
4) Specificity: A specific cause leads to a specific effect.
5) Biological Plausibility: The cause-and-effect relationship must make sense in a biological or logical context.

Example of Causation:
1) Smoking and Lung Cancer: Decades of research have shown a clear causal relationship between smoking and lung cancer. Smoking doesn't just correlate with lung cancer; it causes lung cancer, as smoking introduces harmful substances into the lungs that can damage cells and lead to cancer.
2) Exercise and Health: Regular exercise causes improved cardiovascular health. The relationship is supported by scientific evidence showing that physical activity strengthens the heart and reduces the risk of heart disease.

## Ethics in AI
1. Privacy: Ensure there is no task of personal confidential information.
2. Accountability:
3. Safety: Control over what data is being used and
4. Transparency: Ensure compliance with human moralities.
5. Respect for human values: Ensuring sensitivity to different cultural norms and values.
6. Fairness: No discrimination based on gender, race, caste or cred 
